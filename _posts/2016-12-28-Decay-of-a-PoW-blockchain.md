---
layout: post
tags: PoW 
date: 2016-12-28
title: Decay of a PoW blockchain
published: true
---






What I love mathematics for is how easy questions can slowly unravel into very complicated solutions. I love locally trivial objects that are nontrivial globally (would love to classify them!).



Here's blockchain-related example of such a question: if a PoW network splits into $N$ segments, how can I tell if I mine with the majority? 


<!--more-->



## Problem statement

A certain number of network nodes is involved in digital currency mining. Due to a technical malfunction — e.g. a break in a backbone cable — the network splits temporarily into two segments. In this case the amount of time to solve a block will increase in both segments. In other words, miners will continue to compute blocks but the whole process will be much slower. This case requires to devise a formula and a code based on the amount of time to solve a block which will indicate in what segments miners ended up to be. Therefore, if certain network nodes ended up in a minor segment — i.e. in a segment with non-maximum computational capacity — miners could then decide to stop mining process. In this case we consider the Proof-of-Work algorithm.

With this technical malfunction, all that the network node gets as input is an experimentally obtained sequence of times when new blocks were generated $T_1$, $T_2$, ... . This sequence will specify to the node the percentage a new network makes from the computational capacity of the previous one. If a segment, where the node ended up to be, has more than 50% of computational capacity before the malfunction occurs, then this segment should be considered as major and mining process should be continued. If a segment has less than 50% of computational capacity before the malfunction occurs, then in a few cases — i.e. the network divided in more than 2 segments ($N > 2$) — the node might end up in the major block chain. However, it is a matter of probability. In any case, we will provide all the formulae as well as a required code for the node.


## Solution: TLDR



The malfunction has split a previously single network into disparate segments. Each of the network nodes ended up in a particular segment.
We consider the random variable $\xi$ equal to a number of hashes which should be generated by a segment to solve a block. This random variable is discrete and, as a result, its probability distribution appears to be known. Now we consider the random variable τ which denotes a number of seconds necessary to solve a block. Variables $\xi$ and $\tau$ are clearly interrelated: $\tau = 1 + \text{floor}(\xi/h)$, where $h$ is the unknown hashrate of a segment. Therefore, the probability distribution of variable $\tau$ is also known. However it depends on the unknown h constant which needs to be determined. We shall compute $M\tau$ and $D\tau$. We compute an interval estimate for a parameter $h$ based on a sample $T_1, T_2, ..., T_n : P(h_0 − \varepsilon < h < h_0 + \varepsilon)$. By dividing the interval estimate by $2\varepsilon$ we obtain the probability density function $\rho(h)$. By dividing it by $H$ we obtain the probability density function $\rho(p)$ where $p$ is the post-decay computational power of the segment expressed as a percentage.

There is another problem that should be solved. How to determine if a share of the total computational capacity of all segments is major or minor? If it exceeds 50%, then it's totally major. The same principle allows to determine the share as minor if it is less than $100/N$. But what if the share amounts to 40%? When the network splits into 2 segments ($N = 2$) the second one will definitely be 60% and the first one will be minor. However, when the network splits into 3 segments ($N = 3$), the situation is not that simple: the percentage ratio might be either 40—55—5 or 40—30—30. Politically speaking, in the former case we <<lost>>, in the latter one we <<won>>.

It is clear that with $100/N < p < 50$, considering a segment as major is a matter of probability. We solve this problem for general $N$. At the same time we have to calculate the volume of the part of $(N−2)$-dimensional cube, which was cut out by hyperplane. We obtain the required probability value $V(p)$ denoting majority of a segment. By integrating the product of $\rho(p)$ and $V(p)$ we acquire the probability value to end up in a major segment which depends only on $T_1$, $T_2$, ..., $T_n$.


## Full solution

#### Introduction and notation


Suppose difficulty adjustment occurs every $z$ blocks and the average time to solve a block before the network splits into disparate segments is $T$ minutes. Then difficulty adjustment will occur, on average, every $zT$ minutes. We will consider a simple case where difficulty value wouldn't be adjusted. This case lasts from the moment when the network splits into segments till the moment when a node determines a segment as major or minor. Let $\Delta$ be a difficulty value before the split occurs.

Let $H$ be a network hashrate prior to the split, $h$ be a hashrate of a segment under examination, $h_1$, $h_2$, ..., $h_{N−1}$ be hashrates of remaining $(N − 1)$ elements of the network. It is clear that


$$H = h + \sum\limits_{i=1}^{N-1} h_i$$


After the split occurred, miners will continue to solve blocks with the initial difficulty value $\Delta$. In a segment under examination the average time to solve a block will be $$T \cdot \frac{H}{h}$$ minutes

In the Proof-of-Work algorithm the mining process is a matter of probability. The so-called difficulty value is adjusted to the computational capacity of the network in a way that allows miners to solve a block on average every $T$ minutes. Therefore, the difficulty value directly reflects the amount of computational capacity. Miners search for a valid hash. But since it is unique they are less likely to generate one, let's consider the discovery of a valid hash a success. Let s  be the probability of success in calculating a hash. Then the probability of failure will be $(1 − s)$.

Let $T_1$, $T_2$, ... be times (in seconds) when first, second etc blocks are solved. 

As we know, PoW mining is probabilistic --- one can speak only probability that the next hash will solve the block. Let random variable $\tau$ be a number of seconds it takes to solve a block.


#### Interval estimate for $h$

###### Random variable $\xi$ and its probability distribution

Note that the problem-specific events are actually discrete. Miners compute hashes until they find a valid one. After that the process is repeated. So instead of variable $T_i$ we use $x_i$ which is more appropriate for understanding of the whole process. $\xi$ is a number of hashes calculated by miners of a segment in order to find a $i$ hash and counted from the moment when the hash $(i − 1)$ was calculated. In other words, $\xi$ is an interval between two successful hash calculations expressed in a number of attempts rather than in minutes. Let the random variable $\xi$ be a number of hashes which must be calculated to solve a block.

The benefit of using the random variable $\xi$ is that its probability distribution is known. In fact, the probability for $\xi = 1$ is equal to $s$. The probability for $\xi = 2$ is equal to $s(1−s)$. The probability for $\xi = 3$ is equal to $s(1 − s)^2$. And so on.

Why is that? In fact, it is the third hash that will solve a block. The first two attempts will fail. This case generates a simple event "first hash — failure, second hash — failure, third hash — success". The events are independent and thus, in accordance with the multiplication rule the probability of the event is $(1 − s) \cdot (1 − s) \cdot s = s(1 − s)^2$.

As it should have turned out these probabilities are being summed up to

$$ \nonumber s + s(1-s) + s(1-s)^2 + ... = s \Big( 1 + (1-s) + (1-s)^2 + ... \Big) = s \cdot \frac{1}{1-(1-s)} = s \cdot \frac{1}{s} = 1$$

We calculate the expected value of the random variable:

$$M\xi = 1 \cdot s + 2 \cdot s(1-s) + 3 \cdot s(1-s)^2 + ... = s (1 + 2(1-s) + 3 (1-s)^2 + 4 (1-s)^3 + ...) = $$ $$= -s ( 1 + (1-s) + (1-s)^2 + (1-s)^3 + (1-s)^4 + ... )' = -s \cdot (\frac{1}{1-(1-s)})' =$$ $$= -s \cdot (\frac{1}{s})' = -s \cdot (- \frac{1}{s^2}) = \frac{1}{s}$$

A block is most likely to be solved by a first hash ($s > s(1 − s) > s(1 − s)^2 > ...$). However, the multiplier $(1−s)$ is so close to $1$ and $s$ value is so small that there will be no bias toward the first hash. The case shows that the expected value of hashes that miners will compute to solve a block is equal to $1/s$. The $s$ value is usually very small, and therefore, the $1/s$ value is very large. To illustrate this point, here is a current $s$ value in the Bitcoin network registered in January 2017:
$$s = 0.0000000000000000000007506940164459079$$ 

Once again, the probability distribution of random variable $\xi$ is known. It goes as follows:
$P(\xi = 1) = s, P(\xi = 2) = s(1 − s), P(\xi = 3) = s(1 − s)^2$ and so on.

We'll try to understand what affects the $s$ probability. The probability to solve a block depends on the percentage of all possible hashes that meet the Proof-of-Work condition:

$$s = \frac{\text{current target}}{2^{256}}$$

Taking into account the unambiguous link\footnote{Mining _difficulty_ and _target_ are interrelated: $$\text{difficulty} = \frac{C}{\text{target}}$$ The creator of Bitcoin has chosen the following value for $C = \text{0xffff}\underbrace{00...0}^{\text{51 times}} = 15 \cdot 16^{55} + 15 \cdot 16^{54} + 15 \cdot 16^{53} + 15 \cdot 16^{52} =$ $= 16 \cdot 2^{220} + 16 \cdot 2^{216} + 16 \cdot 2^{212} + 16 \cdot 2^{208} - 2^{220} - 2^{216} - 2^{212} - 2^{208} =  2^{224} - 2^{208} \approx 2^{224}$} between target and difficulty, the result will be: $$ \label{sdiff} s = \frac{\text{current target}}{2^{256}} =\frac{2^{224}/\text{current difficulty}}{2^{256}} = \frac{1}{2^{32} \cdot \text{current difficulty}} $$

It is clear that the probability to solve a block depends solely on difficulty value. Remember that we had previously made the assumption that, from the moment when the network split into segments till the moment when a node determines a segment as major or minor, the \textit{difficulty} value was not adjusted. With this assumption $s$ is a known constant.


#### Random variable $\tau$ and its probability distribution


Time to solve a block is definitely linked to a number of hashes that had to be calculated. Let $T_i$ be an amount of time to generate a block and $x_i$ be a number of hashes calculated for this purpose. Let $h$ (hash/s) be hash rate of the entire segment. Therefore, $$x_i = h T_i.$$ It is true that the previously considered random variables $\tau$ and $\xi$ are interrelated: $$\label{xitau}\xi = h \tau.$$

$\tau$ is a discrete random variable. In fact, any existing cryptoplatform measures time to solve a block (timestamp) in Unix-time, i.e. in seconds. There is simply no split into smaller time intervals. And there is no sense in doing so since the information about computing a block spreads to a network's segment and its miners in one second.

Unlike the variables $\xi$ and $\tau$, $h$ is a continuous and non-random one. Original equipment manufacturers (OEM) are not able to determine a whole number of hashes generated by their hardware. In addition, the equipment deterioration can slow down the process of calculations. For example, the equipment will compute a few billion hashes not in one second, but in one second and a little bit more. In this case a number of hashes per second is definitely fractional.

Using (\ref{xitau}) and the previously discussed probability distribution of the random variable $\xi$, we record the probability distribution of the random variable $\tau$. The probability that a new block has a timestamp one second greater than the previous one, equals to 

$$P(\tau = 1) = \sum\limits_{i=0}^{h-1} s(1-s)^i = s \frac{1 - (1-s)^h}{1 - (1-s)} = 1 - (1-s)^h$$

It is clear how we calculated it. We added the probabilities of finding a valid hash among all the attempts made in the first second.

The probability that a new block has a timestamp two seconds greater than the previous one, equals to 

$$P(\tau = 2) = \sum\limits_{i=h}^{2h-1} s(1-s)^i = (1-s)^h P(0) = (1-s)^h - (1-s)^{2h}$$

The situation is the same. In the 2nd second of attempts to generate a block, $h$ hashes will be calculated. And in the case of serial numbers these will be hashes from $(h+1)$ to $(2h)$.




Probability that a new block will be mined 3 seconds after previous one:

$$P(\tau = 3) = \sum\limits_{i=2h}^{3h-1} s(1-s)^i = (1-s)^h P(1) = (1-s)^{2h} P(0) = (1-s)^{2h} - (1-s)^{3h}$$



Now we calculate $M\tau$ and $D\tau$.



$$M\tau = 1 \cdot \left( 1 - (1-s)^h \right) + 2 \cdot \left( (1-s)^h - (1-s)^{2h} \right) + 3 \cdot \left( (1-s)^{2h} - (1-s)^{3h} \right) + ...  =$$ $$= 1 + (1-s)^h + (1-s)^{2h} + (1-s)^{3h} + ... = \frac{1}{1 - (1-s)^h} = \frac{1}{1 - x}$$

(Here we introduced the notation $(1-s)^h \equiv x$.)

$$M\tau^2 = 1 \cdot \left( 1 - (1-s)^h \right) + 4 \cdot \left( (1-s)^h - (1-s)^{2h} \right) + 9 \cdot \left( (1-s)^{2h} - (1-s)^{3h} \right) + ...  =$$ $$= 1 + 3 (1-s)^h + 5 (1-s)^{2h} + 7 (1-s)^{3h} + ... = \sum\limits_{j=0}^{\infty} (2j+1) \left( (1-s)^h \right)^j $$ 



$$ = \sum\limits_{j=0}^{\infty} (2j+1) x^j = 2\sum\limits_{j=0}^{\infty} j x^j + \sum\limits_{j=0}^{\infty}  x^j = 2x\sum\limits_{j=0}^{\infty} j x^{j-1} + \frac{1}{1-x} = 2x\sum\limits_{j=0}^{\infty} \left(x^j \right)' + \frac{1}{1-x} = $$ $$= 2x  \left(\sum\limits_{j=0}^{\infty} x^j \right)' + \frac{1}{1-x} = 2x  \left( \frac{1}{1-x} \right)' + \frac{1}{1-x} = 2x \cdot\frac{1}{(1-x)^2} + \frac{1}{1-x} = \frac{2x + (1-x)}{(1-x)^2} = \frac{1+x}{(1-x)^2}$$

$$D\tau = M\tau^2 - (M\tau)^2 = \frac{1+x}{(1-x)^2} - \left(\frac{1}{1-x}\right)^2 = \frac{x}{(1-x)^2} $$

<!-- $$D\tau = \frac{\left(1-s\right)^h}{\left(1-\left(1-s\right)^h\right)^2}$$ -->




#### Finding interval estimate

Our plan is to calculate the probability $P(h_0 - \varepsilon < h < h_0 + \varepsilon)$, divide the result by $2\varepsilon$ and then pass on to the limit $\varepsilon \to 0$. Thus we obtain a probabilistic estimate that $h$ is the same as $h_0$.

$T_1$, $T_2$, ..., $T_n$ are experimentally measured times when blocks were solved ($n$ equally distributed independent random variables).

In order to compute the interval estimation for the parameter $h$, having experimentally known $T_1$, $T_2$, ..., $T_n$, we need to choose a function $g(\vec T, h)$ which meets two conditions\footnote{Functions having this property are called _central statistics_.}: 

* it depends monotonically on $h$
* its distribution function does not depend on $h$.

If you take $g(\vec T, h)$, which does not meet the second condition, the answer for $P(h_0 - \varepsilon < h < h_0 + \varepsilon)$ will depend on $h$. It means the failure to compute the interval estimation.








Most probability distributions do not allow to construct this function, and the probability distribution for the random variable $\tau$ is no exception. So we have to use the central limit theorem. It establishes that (1) the sum $n$ of independent and identically distributed random variables tends toward a normal distribution with increasing $n$; (2) the expected value of the normal distribution equals to $n$ expected values of the random variables; (3) the variance of the normal distribution equals to $n$ variances of random variables. In other words, 

$$T_1 + T_2 + ... + T_n \xrightarrow{n\to\infty} N(n \cdot M\tau, n \cdot D\tau)$$

We consider a random variable $$\eta = \frac{\sum\limits_{i=1}^n T_i - n M\tau}{\sqrt{n D\tau}}.$$ Here I would like to denote $\overline{T} = (T_1 + T_2 + ... T_n)/n$ for convenience. It tends to a _standard_ normal distribution, i.e. a normal distribution of $N(0,1)$. In fact, using the properties of expected value and variance, we get 

$$M\eta = M\left( \frac{\sum\limits_{i=1}^n T_i - n M\tau}{\sqrt{n D\tau}} \right) = M\left( \frac{\sum\limits_{i=1}^n T_i }{\sqrt{n D\tau}} \right) - M\left( \frac{n M\tau}{\sqrt{n D\tau}} \right) \xrightarrow{n\to\infty}  \frac{n \cdot M\tau}{\sqrt{n D\tau}} - \frac{n M\tau}{\sqrt{n D\tau}}   = 0$$

$$D\eta = D\left( \frac{\sum\limits_{i=1}^n T_i - n M\tau}{\sqrt{n D\tau}} \right) = \frac{1}{nD\tau} D\left(\sum\limits_{i=1}^n T_i - n M\tau \right) = \frac{1}{nD\tau} D\left(\sum\limits_{i=1}^n T_i \right) \xrightarrow{n\to\infty} \frac{1}{nD\tau} \cdot (n \cdot D\tau) = 1.$$

According to the central limit theorem, the variable $(n\overline{T} - nM\tau)/\sqrt{n D\tau}$ tends to $N(0,1)$.

We substitute $M\tau$ and $D\tau$ into the formula:

$$ \label{sqrtx}\eta = \frac{n \overline{T} - n M\tau}{\sqrt{n D\tau}} = \frac{n \overline{T} - \frac{n}{1-x}}{\sqrt{n \frac{x}{(1-x)^2}}} = \sqrt{n} \frac{\overline{T} (1-x) - 1}{\sqrt{x}} \xrightarrow{n\to\infty} N(0,1) $$


The distribution function of the random variable $\eta$ fortunately does not depend on $h$ (and does not depend on any parameter at all). That is why for any $a$ and $b$ we can easily find an answer for $P(a< \eta< b)$. Next, our plan is to transform $P(a< \eta < b)$ into $P(c < x < d)$ using the dependence of $\eta$ on $x$.

However, it is not that simple since the denominator $\sqrt{x}$ in (\ref{sqrtx}) gets in the way. Fortunately we can eliminate it.

According to one of the properties of sample moments, the sample mean $\overline{T}$ is a consistent estimator for the expected value of the variable $T$. In other words, if $M|T| < \infty$, $\overline{T} \xrightarrow{p} MT$ as $n\to\infty$. 

In this case $MT = 1/(1-x)$, so 

$$\overline{T} \xrightarrow{p} \frac{1}{1-x} \qquad\qquad \frac{1}{\overline{T}} \xrightarrow{p} 1-x \qquad\qquad 1 - \frac{1}{\overline{T}} \xrightarrow{p} x  $$ 

$$ \sqrt{\frac{\overline{T}-1}{\overline{T}}} \xrightarrow{p} \sqrt{x} \qquad\qquad \sqrt{\frac{\overline{T}}{\overline{T}-1}} \xrightarrow{p} \frac{1}{\sqrt{x}} \qquad\qquad \sqrt{\frac{x\overline{T}}{\overline{T}-1}} \xrightarrow{p} 1$$ 


We use one of the properties of the weak convergence: if $\xi_n \xrightarrow{p} 1$ and $\eta_n \to \eta$, then $\xi_n \cdot \eta_n \to \eta$. Thus

$$\sqrt{\frac{x\overline{T}}{\overline{T}-1}} \cdot \sqrt{n} \frac{\overline{T} (1-x) - 1}{\sqrt{x}} = \sqrt{n} \frac{\overline{T} (1-x) - 1}{\sqrt{(\overline{T} - 1)/\overline{T}}} \xrightarrow{n\to\infty} \eta \in N(0,1) $$


$$g(\vec T, h) = \sqrt{n} \frac{\overline{T} (1-x) - 1}{\sqrt{(\overline{T} - 1)/\overline{T}}} \qquad \qquad  g(\vec T, h) \xrightarrow{n\to\infty} N(0,1)$$

Excellent! We finally obtained the function $g(\vec T, h)$, that satisfies the two needed properties. Indeed, it depends monotonically on the unknown $h$ (which resides inside $x$ since $x = (1-s)^h$), and its distribution function does not depend on the unknown parameter $h$. The function itself is a random variable since it depends on random variables $T_1$, $T_2$, ..., $T_n$. We assume that $n$ is large enough to consider its probability distribution standard normal ($N(0,1)$). We also introduce $\eta$; by definition, $\eta \equiv g(\vec T, h)$.

Thus, $F_{\eta} (z) \equiv P(\eta < z)$ is known and does not depend on the parameter $h$. As a result, we can find any $P(a < \eta < b)$ for any $a$ and $b$, and this probability will not depend on $h$:



$$P(a < \eta < b)  = P(\eta < b) - P(\eta < a) = \int\limits_{-\infty}^b \frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2}} dy - \int\limits_{-\infty}^a \frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2}} dy = $$ 

$$ = \left( \int\limits_{-\infty}^0 \frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2}} dy + \int\limits_0^b \frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2}} dy \right) - \left( \int\limits_{-\infty}^0 \frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2}} dy + \int\limits_0^a \frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2}} dy \right)  = $$ 

$$ = \int\limits_0^b \frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2}} dy - \int\limits_0^a \frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2}} dy = \frac{1}{\sqrt{\pi}} \int\limits_0^{b/\sqrt{2}}  e^{-\frac{y^2}{2}} d\left(\frac{y}{\sqrt{2}}\right) - \frac{1}{\sqrt{\pi}} \int\limits_0^{a/\sqrt{2}}  e^{-\frac{y^2}{2}} d\left(\frac{y}{\sqrt{2}}\right) =  \frac{1}{2} \left( \text{erf}\left(\frac{b}{\sqrt{2}}\right) - \text{erf}\left(\frac{a}{\sqrt{2}}\right) \right)$$

We used [Gauss error function](https://en.wikipedia.org/wiki/Error_function) to rewrite the answer in a more compact form.

Once again,

$$\label{etaerf} P(a < \eta < b)  = \frac{1}{2} \left( \text{erf}\left(\frac{b}{\sqrt{2}}\right) - \text{erf}\left(\frac{a}{\sqrt{2}}\right) \right) $$

Now we recall that our goal is to obtain the interval estimate for variable $h$ rather than $\eta$. Solving the inequality with respect to $h$, we obtain


 $$P(a < \eta < b) = P\left(a < \sqrt{n} \frac{\overline{T} (1-x) - 1}{\sqrt{(\overline{T}-1)/\overline{T}}} < b \right) =$$ $$= P \left(a \sqrt{\frac{\overline{T} - 1}{n\overline{T}}} < \overline{T} (1-x) - 1 < b \sqrt{\frac{\overline{T} - 1}{n\overline{T}}} \right) = $$ $$= P \left( \frac{1 + a\sqrt{\frac{\overline{T} - 1}{n\overline{T}}} }{\overline{T}}    < 1 - x < \frac{1 + b\sqrt{\frac{\overline{T} - 1}{n\overline{T}}} }{\overline{T}}  \right) =$$ $$= P \left(  \frac{( \overline{T} - 1) - b\sqrt{\frac{\overline{T} - 1}{n\overline{T}}} }{\overline{T}}  < x < \frac{( \overline{T} - 1) - a\sqrt{\frac{\overline{T} - 1}{n\overline{T}}} }{\overline{T}}  \right)= $$ $$= P \left( \log_{1-s} \left( \frac{( \overline{T} - 1) - a\sqrt{\frac{\overline{T} - 1}{n\overline{T}}} }{\overline{T}} \right) < h < \log_{1-s} \left( \frac{( \overline{T} - 1) - b\sqrt{\frac{\overline{T} - 1}{n\overline{T}}} }{\overline{T}} \right) \right) $$ 
 
Thus, according to (\ref{etaerf}), we obtain

$$ \label{hestimated} P \left( \log_{1-s} \left( \frac{( \overline{T} - 1) - a\sqrt{\frac{\overline{T} - 1}{n\overline{T}}} }{\overline{T}} \right) < h < \log_{1-s} \left( \frac{( \overline{T} - 1) - b\sqrt{\frac{\overline{T} - 1}{n\overline{T}}} }{\overline{T}} \right) \right)  =  \frac{1}{2} \left( \text{erf}\left(\frac{b}{\sqrt{2}}\right) - \text{erf}\left(\frac{a}{\sqrt{2}}\right) \right)  $$

We are close to our goal --- we obtained the interval estimate for parameter $h$. Now we recall that we were interested in the interval estimate of $P(h_0 - \varepsilon < h < h_0 + \varepsilon)$. We select random numbers $a$ and $b$ so that the left part of (\ref{hestimated}) equals to $(h_0 - \varepsilon)$ and its right part equals to $(h_0 + \varepsilon)$.


$$
\left\{ \begin{array}{ll}
        \log_{1-s} \left( \frac{( \overline{T} - 1) - a\sqrt{\frac{\overline{T} - 1}{n\overline{T}}} }{\overline{T}} \right) = h_0 - \varepsilon \\
        \log_{1-s} \left( \frac{( \overline{T} - 1) - b\sqrt{\frac{\overline{T} - 1}{n\overline{T}}} }{\overline{T}} \right) = h_0 + \varepsilon \\
        \end{array} \right. \Longrightarrow 
\left\{ \begin{array}{ll}
        \frac{( \overline{T} - 1) - a\sqrt{\frac{\overline{T} - 1}{n\overline{T}}} }{\overline{T}} = (1-s)^{h_0 - \varepsilon} \\
        \frac{( \overline{T} - 1) - b\sqrt{\frac{\overline{T} - 1}{n\overline{T}}} }{\overline{T}} = (1-s)^{h_0 + \varepsilon} \\
        \end{array} \right. \Longrightarrow $$ $$
\Longrightarrow\left\{ \begin{array}{ll}
        ( \overline{T} - 1) - \overline{T} (1-s)^{h_0 - \varepsilon} = a\sqrt{\frac{\overline{T} - 1}{n\overline{T}}} \\
        ( \overline{T} - 1) - \overline{T} (1-s)^{h_0 + \varepsilon} = b\sqrt{\frac{\overline{T} - 1}{n\overline{T}}}  \\
        \end{array} \right. \Longrightarrow 
\left\{ \begin{array}{ll}
       a = \left( (\overline{T} - 1) - \overline{T} (1-s)^{h_0 - \varepsilon} \right) \sqrt{\frac{n\overline{T}}{\overline{T}-1}} \\
       b = \left( (\overline{T} - 1) - \overline{T} (1-s)^{h_0 + \varepsilon} \right) \sqrt{\frac{n\overline{T}}{\overline{T}-1}} \\
        \end{array} \right. $$
        
        
We plug found $a$ and $b$ into (\ref{hestimated}):

$$ \label{withepsilon} P(h_0 - \varepsilon < h < h_0 + \varepsilon) = \frac{1}{2}  \text{erf} \left( \left( (\overline{T} - 1) - \overline{T} (1-s)^{h_0 + \varepsilon} \right) \sqrt{\frac{n\overline{T}}{2(\overline{T}-1)}}  \right) - \frac{1}{2} \text{erf} \left( \left( (\overline{T} - 1) - \overline{T} (1-s)^{h_0 - \varepsilon} \right) \sqrt{\frac{n\overline{T}}{2(\overline{T}-1)}} \right) $$

We found the probability that the open interval $(h_0 - \varepsilon, h_0 + \varepsilon)$ includes the variable $h$. 

To obtain the probability density, the expression for $P(h_0 - \varepsilon < h < h_0 + \varepsilon)$ should be divided by $2\varepsilon$ (the length of the closed interval $[h_0 - \varepsilon, h_0 + \varepsilon]$) with limit $\varepsilon \to 0$ taken then. This will provide us with $\rho (h)$.

Therefore, we should find the limit

$$L = \lim\limits_{\varepsilon \to 0} \frac{\frac{1}{2}  \text{erf}\left( \left( (\overline{T} - 1) - \overline{T} (1-s)^{h_0 + \varepsilon} \right) \sqrt{\frac{n\overline{T}}{2(\overline{T}-1)}}  \right) - \frac{1}{2} \text{erf}\left( \left( (\overline{T} - 1) - \overline{T} (1-s)^{h_0 - \varepsilon} \right) \sqrt{\frac{n\overline{T}}{2(\overline{T}-1)}} \right)}{2\varepsilon} = $$ 

$$=  \lim\limits_{\varepsilon \to 0} \frac{\text{erf} \left(p - q (1-s)^{\varepsilon} \right) - \text{erf} \left(p - q (1-s)^{-\varepsilon} \right)}{4\varepsilon},$$ 

where from the context it is clear what we denoted by $p$ and $q$.

We are going to work a bit on the numerator of the limit. Using the [Taylor series of the error function](https://en.wikipedia.org/wiki/Error_function), we find:


$$\text{erf} \left(p - q (1-s)^{\varepsilon} \right) - \text{erf} \left(p - q (1-s)^{-\varepsilon} \right) = \frac{2}{\sqrt{\pi}} \sum\limits_{n=0}^{\infty} \frac{(-1)^n}{n! (2n+1)} \left[ \left( p - q (1-s)^{\varepsilon} \right)^{2n+1}  - \left( p - q (1-s)^{-\varepsilon} \right)^{2n+1} \right] = $$ $$= \frac{2}{\sqrt{\pi}} \sum\limits_{n=0}^{\infty} \frac{(-1)^n}{n! (2n+1)} \left[ \left( p - q - \varepsilon q \ln(1-s) + o(\varepsilon) \right)^{2n+1}  - \left( p - q + \varepsilon q \ln(1-s) + o(\varepsilon) \right)^{2n+1} \right] = $$ $$= \frac{2}{\sqrt{\pi}} \sum\limits_{n=0}^{\infty} \frac{(-1)^n}{n! (2n+1)} \left[ (2n+1) (p-q)^{2n} (-\varepsilon q \ln(1-s)) - (2n+1) (p-q)^{2n} \varepsilon q \ln(1-s) + o(\varepsilon) \right] = $$
$$= \frac{-4\varepsilon q \ln(1-s)}{\sqrt{\pi}} \sum\limits_{n=0}^{\infty} \frac{(-1)^n}{n!} (p-q)^{2n}  + o(\varepsilon)= \frac{-4\varepsilon q \ln(1-s)}{\sqrt{\pi}} \sum\limits_{n=0}^{\infty} \frac{(-1)^n ((p-q)^2)^n}{n!}  + o(\varepsilon) = $$ $$= \frac{-4\varepsilon q \ln(1-s)}{\sqrt{\pi}} \sum\limits_{n=0}^{\infty} \frac{(- (p-q)^2)^n}{n!}  + o(\varepsilon) =  \frac{-4\varepsilon q \ln(1-s)}{\sqrt{\pi}} \exp(- (p-q)^2) + o(\varepsilon)$$

We finish calculating the limit:

$$L = \lim\limits_{\varepsilon \to 0} \frac{\frac{-4\varepsilon q \ln(1-s)}{\sqrt{\pi}} \exp(- (p-q)^2) + o(\varepsilon)}{4\varepsilon} =  \frac{-q \ln(1-s)}{\sqrt{\pi}} \exp(- (p-q)^2).$$ 

Substituting $p$ and $q$ with their values, we obtain 

$$ L =  \frac{-\ln(1-s)}{\sqrt{\pi}} \cdot \overline{T} (1-s)^{h_0} \sqrt{\frac{n\overline{T}}{2(\overline{T}-1)}}  \cdot \exp\left(- \frac{n\overline{T}}{2(\overline{T}-1)} (\overline{T} - 1 - \overline{T} (1-s)^{h_0})^2 \right)  $$ 

$$ \label{ocenka} L =  \frac{-\ln(1-s)}{\sqrt{\pi}} \cdot  (1-s)^{h_0} \sqrt{\frac{n\overline{T}^3}{2(\overline{T}-1)}}  \cdot \exp\left(- \frac{n\overline{T}^3}{2(\overline{T}-1)} \left(\frac{\overline{T} - 1}{\overline{T}} - (1-s)^{h_0}\right)^2 \right)  $$




#### Computing interval estimate for share of computational capacity of a segment

Computing the interval estimate for the unknown $h$ is an obvious thing. But since a node determines a segment as major or minor it is more convenient to provide the interval estimate of the variable $p$. The variable $p$ denotes the share of computational capacity of a segment (compared to the capacity of the entire network before the split occurred) that is expressed as a percentage. Thus 

$$ p = \frac{h}{H} \cdot 100 $$ 

(By definition, $0 < p < 100$.)

We have 

$$P(h_0 - \varepsilon < h < h_0 + \varepsilon) = P\left(h_0 - \varepsilon < \frac{pH}{100} < h_0 + \varepsilon\right) = P\left( \frac{100}{H} h_0 - \frac{100}{H} \varepsilon < p < \frac{100}{H} h_0 + \frac{100}{H} \varepsilon \right) = P(p_0 - \epsilon < p < p_0 + \epsilon)$$ 

(Here we introduced new variables $p_0 = \frac{100}{H} h_0$, $\epsilon = \frac{100}{H} \varepsilon$.) Then

$$\lim\limits_{\epsilon \to 0} \frac{P(p_0 - \epsilon < p < p_0 + \epsilon)}{2\epsilon} = \lim\limits_{\epsilon \to 0} \frac{P(h_0 - \varepsilon < h < h_0 + \varepsilon)}{2\epsilon} =$$ 

$$= \lim\limits_{\epsilon \to 0} \frac{P(h_0 - \varepsilon < h < h_0 + \varepsilon)}{2\varepsilon} \frac{H}{100} = \lim\limits_{\varepsilon \to 0} \frac{P(h_0 - \varepsilon < h < h_0 + \varepsilon)}{2\varepsilon} \frac{H}{100} = $$ 

$$=  \frac{-\ln(1-s)}{\sqrt{\pi}} \cdot  (1-s)^{h_0} \sqrt{\frac{n\overline{T}^3}{2(\overline{T}-1)}}  \cdot \exp\left(- \frac{n\overline{T}^3}{2(\overline{T}-1)} \left(\frac{\overline{T} - 1}{\overline{T}} - (1-s)^{h_0}\right)^2 \right) \cdot  \frac{H}{100} = $$ 

$$=  \frac{-\ln(1-s)}{\sqrt{\pi}} \cdot  (1-s)^{\frac{p_0 H}{100}} \sqrt{\frac{n\overline{T}^3}{2(\overline{T}-1)}}  \cdot \exp\left(- \frac{n\overline{T}^3}{2(\overline{T}-1)} \left(\frac{\overline{T} - 1}{\overline{T}} - (1-s)^{\frac{p_0 H}{100}}\right)^2 \right) \cdot  \frac{H}{100}$$


Or, which is the same,

$$ \label{ocenkap} \boxed{ \rho(p) =  \frac{-\ln(1-s)}{\sqrt{\pi}} \cdot (1-s)^{\frac{pH}{100}} \sqrt{\frac{n\overline{T}^3}{2(\overline{T}-1)}}  \cdot \exp\left(- \frac{n\overline{T}^3}{2(\overline{T}-1)} \left(\frac{\overline{T} - 1}{\overline{T}} - (1-s)^{\frac{pH}{100}} \right)^2 \right) \cdot  \frac{H}{100} } \quad\quad $$

This is the required formula for the probability density function that determines if the unknown variable $p$ gets into the interval $(p_0 - \epsilon, p_0 + \epsilon)$, $\epsilon \to 0$.




#### Validity check

Let's check the validity of probability density distribution, i.e. let's check that 

$$\int\limits_0^{100} \rho(p) dp = 1$$

We have: $$I = \int\limits_0^{100} \frac{-\ln(1-s)}{\sqrt{\pi}} \cdot (1-s)^{\frac{pH}{100}} \sqrt{\frac{n\overline{T}^3}{2(\overline{T}-1)}}  \cdot \exp\left(- \frac{n\overline{T}^3}{2(\overline{T}-1)} \left(\frac{\overline{T} - 1}{\overline{T}} - (1-s)^{\frac{pH}{100}} \right)^2 \right) \cdot  \frac{H}{100} dp = $$ $$= \left[ u = \frac{pH}{100} \right] = \frac{-\ln(1-s)}{\sqrt{\pi}} \int\limits_0^{H}  (1-s)^u \sqrt{\frac{n\overline{T}^3}{2(\overline{T}-1)}}  \cdot \exp\left(- \frac{n\overline{T}^3}{2(\overline{T}-1)} \left(\frac{\overline{T} - 1}{\overline{T}} - (1-s)^u \right)^2 \right) du =$$

$$=  \left[ \beta = \frac{n\overline{T}^3}{2(\overline{T}-1)} \right]  = \left[ \gamma = \frac{\overline{T}-1}{\overline{T}} \right] = \frac{-\ln(1-s)}{\sqrt{\pi}}  \int\limits_0^{H}  (1-s)^u \sqrt{\beta}  \cdot \exp\left(- \beta \left(\gamma - (1-s)^u \right)^2 \right) du = $$ $$= \left[ c^u du = \frac{d(c^u)}{\ln(c)} \right] = \frac{-\ln(1-s)}{\sqrt{\pi}}  \int\limits_1^{(1-s)^H}  \sqrt{\beta}  \cdot \exp\left(- \beta \left(\gamma - (1-s)^u \right)^2 \right) \frac{d\left((1-s)^u\right)}{\ln(1-s)} =  \left[ v = (1-s)^u \right] =$$ $$= \frac{-1}{\sqrt{\pi}} \int\limits_1^{(1-s)^H}  \sqrt{\beta}  \cdot \exp\left(- \beta \left(\gamma - v \right)^2 \right) dv = \frac{1}{\sqrt{\pi}} \int\limits_{(1-s)^H}^1  \sqrt{\beta}  \cdot \exp\left(- \beta \left(\gamma - v \right)^2 \right) dv $$

This is Gaussian integral, and our Gauss bell turns out to be narrow one ($\beta$ cannot be less than $120$, see (\ref{betaestimate})). Thus, almost all the integral value are given by $v$ close to $\gamma$. By the definition, $\gamma$ is slightly more than $1$ (i.e. definitely higher than upper integration limit), but it is still up to find out if it is higher than lower integration limit. 

Compare $(1-s)^H \approx 1 - sH$ and $\gamma = 1 - 1/\overline{T}$. 

To do this, recall the definitions: $s$ is the probability to find a valid block by calculating a hash, $H$ is the network hashrate (how many hashes are calculated per second), $T$ is the average block time prior to the decay. $sH$ is expectation value of blocks mined per second prior to the decay, $sHT$ is expectation value of blocks mined per $T$ seconds prior to the decay. By definition of $T$, $sHT = 1$. For large enough $n$, clearly, $\overline{T} > T$, so $sH\overline{T} > 1$. Thus

$$sH\overline{T} > 1 \qquad \longrightarrow \qquad sH > \frac{1}{\overline{T}} \qquad \longrightarrow \qquad 1 - sH < 1 - \frac{1}{\overline{T}}$$ 

Now Taylor series enter the game: since $sH \ll 1$ (proof: $sHT = 1 \to sH = 1/T \to sH \ll 1$), $(1-s)^H \approx 1 - sH$ holds. So $$1 - sH < 1 - \frac{1}{\overline{T}} \qquad \longrightarrow \qquad (1-s)^H < \frac{\overline{T}-1}{\overline{T}} \qquad \longrightarrow \qquad (1-s)^H < \gamma$$

When $\beta \to\infty$, the expression $\sqrt{\beta/\pi} \exp(-\beta(\gamma - v)^2)$ tends to $\delta(\gamma - v)$, where $\delta(...)$ is the [Dirac delta function](https://en.wikipedia.org/wiki/Dirac_delta_function). Thus, for large enough $n$ and $\overline{T}$ $$I \xrightarrow{n\to\infty} \int\limits_{(1-s)^H}^1 \delta(\gamma - v) dv = 1,$$ since $\gamma$ is in $((1-s)^H,1)$.




#### Pictures, pictures

For visibility, we want to provide graphical interpretation of (\ref{ocenkap}). Since (\ref{ocenkap}) has four parameters ($s$, $H$, $n$, $p$), for that purpose, we have to fix at least two of them. We take $s$ and $H$ equal to their current values in Bitcoin network. Given the fact Bitcoin mining difficulty is $\approx 3.1 \cdot 10^{11}$ at the moment, using (\ref{sdiff}) one can obtain

$$s \approx 0.0000000000000000000007506940164459079,$$ 

thus $$-\ln(1-s) \approx 7.50694 \cdot 10^{-22}$$

Current Bitcoin hashrate is also easy to extract from open sources; for late Dec 2016 it is

$$H = 2.4 \cdot 10^{18}$$

hashes per second. Finally, for late Dec 2016 $(1-s)^H \approx 0.998196$. Generally speaking, since $s$ is inverse proportional to $H$, $(1-s)^H$ cannot fluctuate significantly (for huge $H$ the expression $(1-s)^H$ is close to exponent of a small negative constant). 


After $s$ and $H$ are set equal to their current Bitcoin values, the expression (\ref{ocenkap}) depends only on two parameters: $\overline{T}$ and $n$. Let us plot a graph. In case of average block time equal to 20 minutes ($\overline{T} = 20 \cdot 60 = 1200$), here's what we have for probability density:


[ural20.png]


It is how it's supposed to be, since in Bitcoin average block time is $10$ minutes. If block time seems to get localized around $20$ minutes, then the entire network decayed into segments, and our segment constitutes roughly 50% — and this is what our plot shows. Note that the expression (\ref{ocenkap}), used for this plot, does not have average Bitcoin block time, but plot turned out to be correct anyway. This is another correctness sign of (\ref{ocenkap}).

Clearly, the higher $n$, the more confident we are in our estimate of $p$ (the less is variance and the more localized around a certain value is the Gauss bell).

Ok. Now, the next question is: what happens if a small segment of miners breaks away? This happens from time to time when miners turn off their mining rigs under various reasons.

Let's see the probability of us entering the segment of computation power $p$ in case experimentally measured block time is $11$ minutes ($\overline{T} = 11 \cdot 60 = 660$). Since $11$ is pretty close to $10$, one should expect the significant shift of probability density to $p = 100$.

![alt text](https://garkoosha.me/ural11.png)

This is predictable; we can estimate $p>80$ (or, in case of more conservative estimate, $p>60$). Since average block time = $11$ minutes is not much different than the regular $10$ minutes, one needs really high values of $n$ to tell that the decay has happened. Then one could tell with higher reliability that this is abnormal situation and the decay has happened:

![alt text](https://garkoosha.me/ural1111.png)

Ok, done with $11$ minutes average block time case. How does probability distribution look when the average block time is much higher than the expected $10$ minutes --- e.g. $40$ minutes?

Taking $\overline{T} = 40 \cdot 60 = 2400$ and plotting:

![alt text](https://garkoosha.me/ural40.png)

We even had to change the scale to fit this plot. We see $p$ is close to 25%, as it should be. The higher $n$, the narrower Gaussian bell is.

Speaking of scale, how will those plots look for huge $n$?

![alt text](https://garkoosha.me/ural4000.png)

As expected, for huge $n$ there is not much difference between $n$ and $(n-1)$, and all plots coincide. The interval in $p$ where function is significantly above zero, is wide enough ($15 < p < 30$), and there's nothing surprising in the fact that total area under this function is equal to one (as it should be for _total probability_ which is the integral of probability density function). 

Ok, now let's see if our Gauss bell resembles Dirac $\delta$-function in case of large $n$.



![alt text](https://garkoosha.me/ural2017.png)

As expected, for huge $n$ Gauss bell degenerates to a Dirac $\delta$-function. In practice, one shouldn't wait for too many blocks, maximizing $n$; several blocks are enough to tell $p$ is well-localized around a certain value. 




#### Determining a segment as major or minor for different $N$

We recall that N designates a number of disparate segments into which the network divided as a result of a temporary technical malfunction. However, even if the time to solve a block leads to the conclusion that a segment has, for instance, only 40% of the previous computational capacity, if $N > 2$, it does not mean that we ended up in the minor block chain. If there are 20 segments, 40% of the computational capacity of a segment indicate that the block chain is likely to be major.

Unfortunately, in order for a node to determine its segment as major or minor, it must be aware of a number of segments into which the network divided. There is no way to circumvent this requirement. In a real situation a node will have to make a decision based on geographic particularities. Thus, it will have to try to connect to any IP address. To avoid the situation it is necessary to record in advance IP addresses of miners. Therefore, we should compute estimates for each $N$.

What is clear is that when a segment holds more than 50% of the computational capacity, it is definitely major.


##### Case $N=2$

If the initial network splits into two segments, then: 
* if our segment has less than 50% of the computational capacity of the previously single network, it is definitively minor
* if our segment has more than 50% of the computational capacity of the previously single network, it is definitely major. 
Therefore, it is easy to find the probability that the segment is major:

$$ p_{\text{maj}} = \int\limits_{50}^{100} \rho(p) dp $$

##### Case $N=3$

If there are three sectors, the situation is more complicated. Both 35% and 40% can determine a segment as major. Assuming that there is no additional data (in real life it is often possible to estimate how computational capacities might had been distributed across regions with which we have lost connection), we set a problem and solve it. The problem is the following: <<Let $p$% be a share of the computational capacity of a segment. Find the probability that we are in a major blockchain>>.


It is clear that if the computational capacity of the first segment is less than $\frac{100}{3}$% then other segments definitely have a much greater computational capacity. In other words, the blockchain is minor. It is also clear that if the computational capacity of the first segment has more than 50%, the blockchain is major. So the case to consider is $\frac{100}{3} \leqslant p \leqslant 50$.

So we are in a minor blockchain if one of the other two segments has the share of computational capacity that exceeds $p$. And we are in a major blockchain if no segment has the share that exceeds $p$. We denote percentage values of computational capacities of the other two segments by $x_1$ and $x_2$. Since $p + x_1 + x_2 = 100$, the constraint $x_1 + x_2 = 100 - p$ holds.

$$
\left\{ \begin{array}{ll}
        x_1 + x_2 = 100 - p \\
        x_1 < p \\
        x_2 < p \\
        \end{array} \right. 
        \Longrightarrow 
\left\{ \begin{array}{ll}
        x_1 + x_2 = 100 - p \\
        x_1 < p \\
        100 - p - x_1 < p \\
        \end{array} \right.
        \Longrightarrow 
\left\{ \begin{array}{ll}
        x_1 + x_2 = 100 - p \\
        x_1 < p \\
        x_1 > 100-2p \\
        \end{array} \right.
$$

That is, we are in the major chain if and only if $100 - 2p < x_1 < p$. The length of this interval is equal to $p - (100-2p) = 3p - 100$. The value of $x_1$ cannot be less than $0$ and more than $(100-p)$ so the fraction that this interval represents among all the possible values of $x_1$ equals to $(3p-100)/(100-p)$. According to our assumptions, all the possible values of $x_1$ are equally probable, so $(3p-100)/(100-p)$ is the probability that the block chain is major if $\frac{100}{3} \leqslant p \leqslant 50$.

We record the probability that the blockchain is major $$V(p) = \left\{ \begin{array}{ll}
        0, \hskip 20 pt & \text{if} \: p < 100/3 \\
        (3p-100)/(100-p), \hskip 20 pt & \text{if} \: 100/3 \leqslant p \leqslant 50 \\
        1, \hskip 20 pt & \text{if} \: p > 50.
        \end{array} \right.$$
        
In accordance with probability multiplication theorem, we have to integrate the product of $\rho(p)$ and $V(p)$ with respect to $p$ in order to get an answer to the problem (the probability to get into the major sector):
 
$$ p_{\text{maj}} = \int\limits_0^{100} \rho(p) \cdot V(p) \: dp = \int\limits_0^{100/N} \rho(p) \cdot 0 \: dp + \int\limits_{100/N}^{50} \rho(p) \cdot \frac{3p-100}{100-p} \: dp + \int\limits_{50}^{100} \rho(p) \cdot 1 \: dp $$

$$ p_{\text{maj}} = \int\limits_{100/N}^{50} \rho(p) \cdot \frac{3p-100}{100-p} \: dp + \int\limits_{50}^{100} \rho(p) \: dp $$


##### Case $N=4$


As in the last case, if our share of network is less than $25$%, then one of other connected components has computational power bigger than ours, i.e. our segment is minor. Clearly, if our share of network is bigger than $50$%, our segment is major. The only case left to consider is $25 \leqslant p \leqslant 50$.

$$
\left\{ \begin{array}{ll}
        x_1 + x_2 + x_3 = 100 - p \\
        x_1 < p \\
        x_2 < p \\
        x_3 < p \\
        \end{array} \right. 
        \Longrightarrow 
\left\{ \begin{array}{ll}
        x_1 + x_2 + x_3 = 100 - p \\
        x_1 < p \\
        x_2 < p \\
        100-p - x_1 -x_2 < p \\
        \end{array} \right.
        \Longrightarrow 
\left\{ \begin{array}{ll}
        x_1 + x_2 + x_3 = 100 - p \\
        x_1 < p \\
        x_2 < p \\
        x_1 + x_2 > 100-2p \\
        \end{array} \right.
$$

Two independent variables are here: $x_1$ and $x_2$, each of which may have values starting with $0$ but no more than $(100-p)$. If one draws a square with side $(100-p)$, each of its points will be in 1-to-1 correspondence with a pair of values $(x_1, x_2)$, thus uniquely determining hashrates of each of network segments. The set of admissible pairs $(x_1, x_2)$ has the area of $(100-p)^2$; some people would say that \textit{phase space volume} here is equal $(100-p)^2$.

The question of how to satisfy $x_1 < p$, $x_2 < p$, $x_1 + x_2 > 100-2p$ is the question of determining part of the square, bounded by these inequalities. It can be solved either by plane geometry methods (calculating area of the right triangle) or by calculating double integral $\iint\limits_D dx_1 dx_2$ with respect to the area $D = {x_1, x_2|x_1 < p, x_2 < p, x_1 + x_2 > 100-2p}$. 

Instead of calculating this double integral, we move to the case of arbitrary $N$ right away.


##### Arbitrary $N$ case

В общем случае задача сводится ко взятию $(N-2)$-кратного интеграла $\iint\limits_D dx_1 dx_2 ... dx_{N-2}$ по области $$D = {x_1, x_2, ..., x_{N-2} |\:  x_1 < p,\: x_2 < p,\: ...,\: x_{N-2} < p,\: x_1 + x_2 + ... + x_{N-2} > 100-2p  	} .$$


$$
\left\{ \begin{array}{ll}
        x_1 + x_2 + ... + x_{N-1} = 100 - p \\
        x_1 < p \\
        x_2 < p \\
        ... \\
        x_{N-1} < p \\
        \end{array} \right. 
        \Longrightarrow 
\left\{  \begin{array}{ll}
        x_1 + x_2 + ... + x_{N-1} = 100 - p \\
        x_1 < p \\
        x_2 < p \\
        ... \\
        100-p - x_1 -x_2 - ... - x_{N-2} < p \\
        \end{array} \right.
        \Longrightarrow 
\left\{  \begin{array}{ll}
        x_1 + x_2 + ... + x_{N-1} = 100 - p \\
        x_1 < p \\
        x_2 < p \\
        ... \\
        x_1 + x_2 + ... + x_{N-2} > 100-2p \\
        \end{array} \right.
$$

This is the problem of determining volume of section of hypercube after it is crossed by the hyperplane. For unit hypercube

$$
\left\{ 
        \begin{array}{ll}
        x_1 < 1 \\
        x_2 < 1 \\
        ... \\
        x_1 + x_2 + ... + x_{N-2} > \alpha \\
        \end{array} \right.
$$

the solution is fortunately [known](http://math.stackexchange.com/questions/454583/volume-of-cube-section-above-intersection-with-plane): $$\label{unit} V = 1 - \sum\limits_{i=0}^{\text{floor} (\alpha)} (-1)^i C^i_{N-2} \frac{(\alpha - i)^{N-2}}{(N-2)!} $$

Our cube is, however, not unit cube and has edge of $p$. By similarity transformation we turn to unit, to be able to use (\ref{unit}). Clearly, to do this, we scale every of its sides to make them $p$ times less. The coefficient $(100-2p)$, thanks to linearity of $x_1 + x_2 + ... + x_{N-2}$, will be scaled down as well exactly by $p$ times. Thus, $\alpha = (100-2p)/p$.

Applying (\ref{unit}) to our case, and not forgetting to go back to the original cube with side $p$ (to multiply by $p^{N-2}$), we obtain
$$\label{almost} V = p^{N-2} \left( 1 - \sum\limits_{i=0}^{\text{floor} (100/p) - 2} (-1)^i C^i_{N-2} \frac{(\frac{100-2p}{p} - i)^{N-2}}{(N-2)!} \right) $$

To regain the final answer we are about to recall that it's not the quantity but the share of majority-giving outcomes matters. Each of the variables $x_1$, $x_2$, ..., $x_{N-2}$ may have any value starting with $0$ and no more than $(100-p)$, thus giving $(100-p)^{N-2}$ for the volume of possible distributions space.

To obtain the share of cases, when our chain become major one, we have to divide (\ref{almost}) by volume of the entire phase space $(100-p)^{N-2}$:

$$ V(p) = \left(\frac{p}{100-p}\right)^{N-2} \left( 1 - \sum\limits_{i=0}^{\text{floor} (100/p) - 2} (-1)^i C^i_{N-2} \frac{(\frac{100-2p}{p} - i)^{N-2}}{(N-2)!} \right) $$

This is the correct answer for $p$ satisfying $100/N \leqslant p \leqslant 50$ condition. Earlier we have come to the fact that for $p>50$ the probability of being in a major chain is $1$, and for  $p < 100/N$ it is equal to $0$. That said, final answer is

$$ \label{V} \boxed{V(p) = \left\{ \begin{array}{ll}
        0, \hskip 3 pt & \text{если} \: p < 100/N \\
        \left(\frac{p}{100-p}\right)^{N-2} \left( 1 - \sum\limits_{i=0}^{\text{floor} (100/p) - 2} (-1)^i C^i_{N-2} \frac{(\frac{100-2p}{p} - i)^{N-2}}{(N-2)!} \right), \hskip 3 pt & \text{если} \: 100/N \leqslant p \leqslant 50 \\
        1, \hskip 3 pt & \text{если} \: p > 50.
        \end{array} \right.}\quad\quad\quad\quad $$



<img class="fullwidth" src="https://garkoosha.me/plot57.png">



#### Finalization

The final answer for probability is

$$ p_{\text{maj}} = \int\limits_0^{100} \rho(p) V(p) dp $$

At this moment, we could've started writing a code for numeric integrating $\rho(p) V(p)$. But instead, let us have another look at (\ref{ocenkap}). Write it out once again:

$$ \rho(p) = \frac{-\ln(1-s)}{\sqrt{\pi}} \cdot \overline{T} (1-s)^{\frac{pH}{100}} \sqrt{\frac{n\overline{T}}{2(\overline{T}-1)}}  \cdot \exp\left(- \frac{n\overline{T}}{2(\overline{T}-1)} \left(\overline{T} - 1 - \overline{T} (1-s)^{\frac{pH}{100}} \right)^2 \right) = $$ $$= \frac{-\ln(1-s)}{\sqrt{\pi}} \cdot \overline{T} (1-s)^{\frac{pH}{100}} \sqrt{\frac{n\overline{T}}{2(\overline{T}-1)}}  \cdot \exp\left(- \frac{n\overline{T}^3}{2(\overline{T}-1)} \left(\frac{\overline{T} - 1}{\overline{T}} - (1-s)^{\frac{pH}{100}}\right)^2 \right)  $$

It isn't hard to see that $-n\overline{T}^3/(2(\overline{T}-1))$ (multiplier under exponent), is huge. Indeed, no decentralized platform at this point can have block time less than $15$ seconds. When a network disintegrates, block time becomes bigger. For conservative estimate, one has $\overline{T} > 15$. For $n$ one can say $n > 1$. Thus, 

$$ \label{betaestimate}\frac{n\overline{T}^3}{2(\overline{T}-1)} > 120 $$

Using notation 

$$(1-s)^{\frac{pH}{100}} \equiv y, (n\overline{T}^3)/(2(\overline{T}-1)) \equiv \beta, (\overline{T}-1)/\overline{T} \equiv \gamma$$ 

we have 

$$ \rho(y) =  \frac{-\ln(1-s)}{\sqrt{\pi}} \cdot y \sqrt{\beta}  \cdot \exp\left(- \beta (\gamma - y)^2 \right), \hskip 20 pt \beta > 120.$$

Clearly, with $\beta>120$ gauss bell curve is narrow one. For such big $\beta$ this function degenerates to Dirac $\delta$-function:

$$f_k (x) = \frac{k}{\sqrt{\pi}} e^{-(kx)^2} \xrightarrow{k \to\infty} \delta(x).$$

Our problem led to exactly same function, with $\sqrt{\beta}$ standing for $k$. By the definition, $\beta$ tends to infinity for $n\to\infty$. Thus, for very big $n$ $$ \rho(y) =  -\ln(1-s) \cdot y  \cdot \delta(\gamma - y).$$ Returning to variable $p$, one obtains

$$ \label{rhodelta} \rho(p) =  -\ln(1-s) \cdot (1-s)^{\frac{pH}{100}}  \cdot \delta\left(\gamma - (1-s)^{\frac{pH}{100}}\right) $$

It would be nice to get function that directly points to the computational power percent (i.e. proportional to $\delta(p - \text{value})$). We utilize the well-known $\delta$-function property $\delta(f(x)) = \sum\limits_k \delta(x - x_k)/|f'(x_k)|$, where $x_k$ --- simple zeros of $f(x)$. Let $a = (1-s)$, $b = \gamma$, $c = H/100$. We have $$\delta\left(b - a^{cp} \right) = \frac{\delta\left( p - \frac{1}{c} \log_a b \right)}{\left| - c a^{c \frac{1}{c} \log_a b} \ln a \right|} = \frac{\delta\left( p - \frac{1}{c} \log_a b \right)}{\left| - c b \ln a \right|} = \frac{1}{-\ln a} \cdot \frac{\delta\left( p - \frac{1}{c} \log_a b \right)}{bc} $$
(we used monotonicity of $a^{cp}$ here, as well as the fact that meaning of our notations imply $c > 0$, $b > 0$, $\ln a < 0$).

Returning from variables $a$, $b$, $c$ to those with which we started, and substituting the outcome thing to(\ref{rhodelta}), we obtain

$$\rho (p) = -\ln(1-s) \cdot (1-s)^{\frac{pH}{100}} \cdot \frac{1}{-\ln (1-s)} \cdot \frac{\delta\left( p - \frac{100}{H} \log_{1-s} \gamma \right)}{H \gamma/100} = (1-s)^{\frac{p H}{100}} \frac{100}{H \gamma} \delta\left( p - \frac{100}{H} \log_{1-s} \gamma \right)$$

Why did we do all that? The answer is, there is no function that is nicer in integrating than Dirac $\delta$-function: $$\int\limits_{-\infty}^{\infty} f(x) \delta(x - x_0) dx = f(x_0)$$ The answer to an original problem is integral of $\rho(p) V(p)$. The possibility to rephrase one of the multipliers of $\rho(p)$ as a $\delta$-function means that, instead of calculating the integral, one just has to calculate the value of $V(p)$ in some point. Namely,

$$p_{\text{maj}} = \int\limits_0^{100} \rho(p) V(p) dp = \int\limits_0^{100} (1-s)^{\frac{p H}{100}} \frac{100}{H \gamma} \delta\left( p - \frac{100}{H} \log_{1-s} \gamma \right) V(p) dp = $$ $$= (1-s)^{\log_{1-s} \gamma} \frac{100}{H \gamma}  V\left(\frac{100}{H} \log_{1-s} \gamma\right) = \frac{100}{H}  V\left(\frac{100}{H} \log_{1-s} \gamma\right)$$

$$\label{easy}  \boxed{p_{\text{maj}} = \frac{100}{H} V\left(\frac{100}{H} \log_{1-s} \gamma\right)}, $$

 where $V(...)$ stands for evaluation of $V$ at the point $\frac{100}{H} \log_{1-s} \gamma$.


It is important to say that formula (\ref{easy}) is  \textit{approximate} one, that gives good accuracy for big $n$ and $\overline{T}$ (for big $\beta = n\overline{T}^3/(2(\overline{T}-1))$, to be precise). We will leave it for reader which way to choose --- simple or complicated one. Simple is --- integral with $\delta$-function, which led to the expression (\ref{easy}). The complicated one is --- numeric integration of product $\rho(p) V(p)$, which provides precise answer, but also requires splitting $p$ to more and more intervals for growing $\beta$'s. The complicated one does not fit well for big $\beta$'s, and then simple one is the case.
